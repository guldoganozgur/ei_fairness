{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eb71b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as optim\n",
    "\n",
    "from algorithms import *\n",
    "from models import *\n",
    "from dataloaders import *\n",
    "\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8e4a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "dataset = SyntheticDataset(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ca8317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_kde_model_runner(dataset, hp, seeds):\n",
    "    test = {'accuracy':[],\n",
    "            'ei_disparity':[],\n",
    "            'dp_disparity':[],\n",
    "            'eo_disparity':[],\n",
    "            'eodd_disparity':[]}\n",
    "    \n",
    "    train = {'accuracy':[],\n",
    "            'ei_disparity':[],\n",
    "            'dp_disparity':[],\n",
    "            'eo_disparity':[],\n",
    "            'eodd_disparity':[]}\n",
    "    \n",
    "    val = {'accuracy':[],\n",
    "            'ei_disparity':[],\n",
    "            'dp_disparity':[],\n",
    "            'eo_disparity':[],\n",
    "            'eodd_disparity':[]}\n",
    "\n",
    "    def append_res(l,acc,ei,dp,eo,eodd):\n",
    "        l['accuracy'].append(acc)\n",
    "        l['ei_disparity'].append(ei)\n",
    "        l['dp_disparity'].append(dp)\n",
    "        l['eo_disparity'].append(eo)\n",
    "        l['eodd_disparity'].append(eodd)\n",
    "\n",
    "    for i in range(len(seeds)):\n",
    "        print('training seed', seeds[i] ,'started')\n",
    "        random.seed(seeds[i])\n",
    "        np.random.seed(seeds[i])\n",
    "        torch.manual_seed(seeds[i]) \n",
    "\n",
    "        model = logReg(num_features=dataset.XZ_train.shape[1])\n",
    "        model = model.to(device)\n",
    "        \n",
    "        lr = hp['learning_rate']\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        \n",
    "        results = trainer_kde_fair(\n",
    "            model,\n",
    "            dataset,\n",
    "            optimizer,\n",
    "            device,\n",
    "            n_epochs=hp['n_epochs'],\n",
    "            batch_size=hp['batch_size'], \n",
    "            z_blind=False,\n",
    "            fairness=hp['fairness'], \n",
    "            lambda_=hp['lambda_'], \n",
    "            h=hp['h'], \n",
    "            delta_huber=hp['delta_huber'], \n",
    "            optimal_effort=True, \n",
    "            delta_effort=hp['delta_effort']\n",
    "            )\n",
    "        \n",
    "        append_res(train,results.train_acc_hist[-1],results.train_ei_hist[-1],results.train_dp_hist[-1],results.train_eo_hist[-1],results.train_eodd_hist[-1])\n",
    "        append_res(val,results.val_acc,results.val_ei,results.val_dp,results.val_eo,results.val_eodd)\n",
    "        append_res(test,results.test_acc,results.test_ei,results.test_dp,results.test_eo,results.test_eodd)\n",
    "\n",
    "    def get_res(l):\n",
    "        res = {}\n",
    "        res['accuracy_mean'] = np.mean(l['accuracy'])\n",
    "        res['accuracy_var'] = np.std(l['accuracy'])\n",
    "        res['accuracy_list'] = l['accuracy']\n",
    "        res['ei_mean'] = np.mean(l['ei_disparity'])\n",
    "        res['ei_var'] = np.std(l['ei_disparity'])\n",
    "        res['ei_list'] = l['ei_disparity']\n",
    "        res['dp_mean'] = np.mean(l['dp_disparity'])\n",
    "        res['dp_var'] = np.std(l['dp_disparity'])\n",
    "        res['dp_list'] = l['dp_disparity']\n",
    "        res['eo_mean'] = np.mean(l['eo_disparity'])\n",
    "        res['eo_var'] = np.std(l['eo_disparity'])\n",
    "        res['eo_list'] = l['eo_disparity']\n",
    "        res['eodd_mean'] = np.mean(l['eodd_disparity'])\n",
    "        res['eodd_var'] = np.std(l['eodd_disparity'])\n",
    "        res['eodd_list'] = l['eodd_disparity']\n",
    "        return res\n",
    "\n",
    "    res_train = get_res(train)\n",
    "    res_val = get_res(val)\n",
    "    res_test = get_res(test)\n",
    "    print('Training finished for all seeds.')\n",
    "    \n",
    "    return res_train, res_val, res_test\n",
    "\n",
    "\n",
    "def experiment_runner(dataset, SGD_hp, EI_hp, DP_hp, EO_hp, EODD_hp, seeds):\n",
    "    \n",
    "    _, _, SGD = lr_kde_model_runner(dataset, SGD_hp, seeds)\n",
    "    _, _, EI = lr_kde_model_runner(dataset, EI_hp, seeds)\n",
    "    _, _, DP = lr_kde_model_runner(dataset, DP_hp, seeds)\n",
    "    _, _, EO = lr_kde_model_runner(dataset, EO_hp, seeds)\n",
    "    _, _, EODD = lr_kde_model_runner(dataset, EODD_hp, seeds)\n",
    "    \n",
    "    return SGD, EI, DP, EO, EODD\n",
    "\n",
    "def hyperparameter_test(dataset, hp_test, seed=0):\n",
    "    hp = hp_test.copy()\n",
    "    result = []\n",
    "    for i in hp_test['learning_rate']:\n",
    "        for k in hp_test['lambda_']:\n",
    "            c = []\n",
    "            hp['learning_rate'] = i\n",
    "            hp['lambda_'] = k\n",
    "            train, val, _ = lr_kde_model_runner(dataset, hp, seeds=[seed])\n",
    "            c.append(hp['learning_rate'])\n",
    "            c.append(hp['lambda_'])\n",
    "            c.append(train['accuracy_mean'])\n",
    "            c.append(val['accuracy_mean'])\n",
    "            c.append(val['ei_mean'])\n",
    "            c.append(val['dp_mean'])\n",
    "            c.append(val['eo_mean'])\n",
    "            c.append(val['eodd_mean'])\n",
    "            result.append(c)\n",
    "    print(tabulate(result, headers=['learning_rate', 'lambda_', 'accuracy_train', 'accuracy_val','ei', 'dp', 'eo', 'eodd']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf55157e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:10<00:00,  9.56epochs/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "namespace(train_acc_hist=[0.48171875,\n",
       "                          0.413359375,\n",
       "                          0.3690625,\n",
       "                          0.343359375,\n",
       "                          0.32296875,\n",
       "                          0.308203125,\n",
       "                          0.29703125,\n",
       "                          0.28875,\n",
       "                          0.28171875,\n",
       "                          0.27515625,\n",
       "                          0.269765625,\n",
       "                          0.263828125,\n",
       "                          0.25921875,\n",
       "                          0.254609375,\n",
       "                          0.25125,\n",
       "                          0.247578125,\n",
       "                          0.24515625,\n",
       "                          0.241796875,\n",
       "                          0.23890625,\n",
       "                          0.23703125,\n",
       "                          0.234765625,\n",
       "                          0.233515625,\n",
       "                          0.232109375,\n",
       "                          0.23015625,\n",
       "                          0.228203125,\n",
       "                          0.22609375,\n",
       "                          0.22359375,\n",
       "                          0.2215625,\n",
       "                          0.219609375,\n",
       "                          0.2171875,\n",
       "                          0.21546875,\n",
       "                          0.2140625,\n",
       "                          0.212890625,\n",
       "                          0.21140625,\n",
       "                          0.210234375,\n",
       "                          0.20890625,\n",
       "                          0.208046875,\n",
       "                          0.2075,\n",
       "                          0.206875,\n",
       "                          0.206328125,\n",
       "                          0.205703125,\n",
       "                          0.204765625,\n",
       "                          0.20390625,\n",
       "                          0.20296875,\n",
       "                          0.20203125,\n",
       "                          0.20140625,\n",
       "                          0.20046875,\n",
       "                          0.199921875,\n",
       "                          0.199375,\n",
       "                          0.198828125,\n",
       "                          0.198046875,\n",
       "                          0.197421875,\n",
       "                          0.19671875,\n",
       "                          0.1959375,\n",
       "                          0.19515625,\n",
       "                          0.1940625,\n",
       "                          0.192734375,\n",
       "                          0.19140625,\n",
       "                          0.19046875,\n",
       "                          0.189375,\n",
       "                          0.18828125,\n",
       "                          0.18671875,\n",
       "                          0.185625,\n",
       "                          0.184609375,\n",
       "                          0.183203125,\n",
       "                          0.1821875,\n",
       "                          0.18015625,\n",
       "                          0.17828125,\n",
       "                          0.17640625,\n",
       "                          0.174609375,\n",
       "                          0.17203125,\n",
       "                          0.168828125,\n",
       "                          0.166328125,\n",
       "                          0.163515625,\n",
       "                          0.15984375,\n",
       "                          0.157265625,\n",
       "                          0.154453125,\n",
       "                          0.150625,\n",
       "                          0.14625,\n",
       "                          0.141796875,\n",
       "                          0.1378125,\n",
       "                          0.13453125,\n",
       "                          0.130546875,\n",
       "                          0.1253125,\n",
       "                          0.1215625,\n",
       "                          0.116328125,\n",
       "                          0.11140625,\n",
       "                          0.107421875,\n",
       "                          0.10375,\n",
       "                          0.0990625,\n",
       "                          0.094921875,\n",
       "                          0.090703125,\n",
       "                          0.08578125,\n",
       "                          0.081328125,\n",
       "                          0.077421875,\n",
       "                          0.074921875,\n",
       "                          0.072265625,\n",
       "                          0.07,\n",
       "                          0.067578125,\n",
       "                          0.065078125],\n",
       "          train_p_loss_hist=[0.7102966886758805,\n",
       "                             0.8115829336643219,\n",
       "                             0.9285414296388627,\n",
       "                             1.0596277433633805,\n",
       "                             1.202343956232071,\n",
       "                             1.3538865983486175,\n",
       "                             1.5121862375736237,\n",
       "                             1.6752813792228698,\n",
       "                             1.8415814697742463,\n",
       "                             2.0100830709934234,\n",
       "                             2.1796323919296263,\n",
       "                             2.3500302600860596,\n",
       "                             2.5207365012168883,\n",
       "                             2.691553223133087,\n",
       "                             2.8621729946136476,\n",
       "                             3.032635929584503,\n",
       "                             3.2026671624183654,\n",
       "                             3.3722740697860716,\n",
       "                             3.5417044615745543,\n",
       "                             3.710658690929413,\n",
       "                             3.8795388078689577,\n",
       "                             4.047666609287262,\n",
       "                             4.215838255882264,\n",
       "                             4.390269241333008,\n",
       "                             4.564190533161163,\n",
       "                             4.7573368763923645,\n",
       "                             4.949428882598877,\n",
       "                             5.226021084785462,\n",
       "                             5.4742943477630615,\n",
       "                             5.799689364433289,\n",
       "                             6.122718758583069,\n",
       "                             6.5027312612533565,\n",
       "                             6.885707221031189,\n",
       "                             7.384715943336487,\n",
       "                             7.912393803596497,\n",
       "                             8.501841859817505,\n",
       "                             9.055668497085572,\n",
       "                             9.618372716903686,\n",
       "                             10.223773837089539,\n",
       "                             10.91704239845276,\n",
       "                             11.66448154449463,\n",
       "                             12.42725269317627,\n",
       "                             13.238459424972534,\n",
       "                             14.012665958404542,\n",
       "                             14.939520606994629,\n",
       "                             15.789646215438843,\n",
       "                             16.655366363525392,\n",
       "                             17.655181951522827,\n",
       "                             18.50687014579773,\n",
       "                             19.453680486679076,\n",
       "                             20.24029649734497,\n",
       "                             21.102675371170044,\n",
       "                             21.776995582580568,\n",
       "                             22.567593517303468,\n",
       "                             23.400748329162596,\n",
       "                             24.349605522155763,\n",
       "                             25.184968013763427,\n",
       "                             26.004362869262696,\n",
       "                             26.74189027786255,\n",
       "                             27.438801841735838,\n",
       "                             28.226577529907228,\n",
       "                             28.998794269561767,\n",
       "                             29.684051876068114,\n",
       "                             30.340873641967775,\n",
       "                             30.87216693878174,\n",
       "                             31.468836860656737,\n",
       "                             32.12365119934082,\n",
       "                             32.65879697799683,\n",
       "                             33.206548862457275,\n",
       "                             33.727889423370364,\n",
       "                             34.17615915298462,\n",
       "                             34.70209108352661,\n",
       "                             35.18946208953857,\n",
       "                             35.518227882385254,\n",
       "                             35.925888481140134,\n",
       "                             36.35996881484985,\n",
       "                             36.8208639717102,\n",
       "                             37.184160957336424,\n",
       "                             37.494317264556884,\n",
       "                             37.84502202987671,\n",
       "                             38.144619159698486,\n",
       "                             38.38002531051636,\n",
       "                             38.57756465911865,\n",
       "                             38.790001983642576,\n",
       "                             38.98411952972412,\n",
       "                             39.141484107971195,\n",
       "                             39.30140739440918,\n",
       "                             39.43041339874267,\n",
       "                             39.54914291381836,\n",
       "                             39.637980136871334,\n",
       "                             39.689983711242675,\n",
       "                             39.65248355865479,\n",
       "                             39.723488330841064,\n",
       "                             39.7245580291748,\n",
       "                             39.74840568542481,\n",
       "                             39.76156969070435,\n",
       "                             39.770176010131834,\n",
       "                             39.774166412353516,\n",
       "                             39.760632019042966,\n",
       "                             39.794321899414065],\n",
       "          train_f_loss_hist=[0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0,\n",
       "                             0.0],\n",
       "          train_dp_hist=[0.06795396192131453,\n",
       "                         0.040203811324853844,\n",
       "                         0.029723791278243095,\n",
       "                         0.024686228570864333,\n",
       "                         0.02403243008374145,\n",
       "                         0.0235524100371306,\n",
       "                         0.02167490421077589,\n",
       "                         0.02097803760467687,\n",
       "                         0.020451443750987508,\n",
       "                         0.019292838521093425,\n",
       "                         0.019955648107915946,\n",
       "                         0.019271051410175377,\n",
       "                         0.01961960815294672,\n",
       "                         0.017754372234950133,\n",
       "                         0.01732693750987513,\n",
       "                         0.016578238564544212,\n",
       "                         0.016001565215673863,\n",
       "                         0.015412621958445283,\n",
       "                         0.014358434389319052,\n",
       "                         0.014495403104755833,\n",
       "                         0.012967207003476089,\n",
       "                         0.012479175718912883,\n",
       "                         0.012150900122452124,\n",
       "                         0.011417976773581917,\n",
       "                         0.0106850534247116,\n",
       "                         0.010427891452046145,\n",
       "                         0.009767834571022371,\n",
       "                         0.008798783378100805,\n",
       "                         0.008065860029230487,\n",
       "                         0.007327678148206673,\n",
       "                         0.006992391175541179,\n",
       "                         0.006188354202875668,\n",
       "                         0.005780200762363741,\n",
       "                         0.00584255016590296,\n",
       "                         0.005120143881339856,\n",
       "                         0.0043959847527255125,\n",
       "                         0.00430734268841837,\n",
       "                         0.0042222063122135944,\n",
       "                         0.004057192091957651,\n",
       "                         0.0036560500276504593,\n",
       "                         0.003334785807394547,\n",
       "                         0.0028520130549849476,\n",
       "                         0.002133112458524211,\n",
       "                         0.0022805982382683165,\n",
       "                         0.001954075485858686,\n",
       "                         0.0016328112656027738,\n",
       "                         0.001622294201295671,\n",
       "                         0.001538910669142024,\n",
       "                         0.0017697799810396653,\n",
       "                         0.00168464360483489,\n",
       "                         0.0016758793845789155,\n",
       "                         0.0018286236964765168,\n",
       "                         0.001583731632169405,\n",
       "                         0.0014169645678622222,\n",
       "                         0.0012501975035550394,\n",
       "                         0.0007639190630431836,\n",
       "                         0.0001960099344288091,\n",
       "                         0.00010210933796817034,\n",
       "                         6.641057039025178e-05,\n",
       "                         0.0008686946990046351,\n",
       "                         0.0015147288276189386,\n",
       "                         0.0018482629562331931,\n",
       "                         0.0020185357086427436,\n",
       "                         0.0027391891491547193,\n",
       "                         0.0033887289658713904,\n",
       "                         0.0039548852504344145,\n",
       "                         0.005401450663611862,\n",
       "                         0.006528504700584636,\n",
       "                         0.008131320113761942,\n",
       "                         0.00902049346263234,\n",
       "                         0.010711950940116899,\n",
       "                         0.012884428325959818,\n",
       "                         0.014812013647495625,\n",
       "                         0.017057357501184978,\n",
       "                         0.019389590575130344,\n",
       "                         0.020923045208563695,\n",
       "                         0.023173647594406654,\n",
       "                         0.025827144888607934,\n",
       "                         0.028880031403065276,\n",
       "                         0.031858298605624946,\n",
       "                         0.035145560120082076,\n",
       "                         0.0380351852583346,\n",
       "                         0.04101345246089427,\n",
       "                         0.04431824241586346,\n",
       "                         0.04737463461842306,\n",
       "                         0.05162568879364826,\n",
       "                         0.05492872590456632,\n",
       "                         0.058381001639279506,\n",
       "                         0.06215629443829984,\n",
       "                         0.06648372076947384,\n",
       "                         0.07009925788039184,\n",
       "                         0.07506745980802654,\n",
       "                         0.08011729242376364,\n",
       "                         0.08469837503950073,\n",
       "                         0.08934355990282827,\n",
       "                         0.0930319634815927,\n",
       "                         0.09815641540922737,\n",
       "                         0.10145945252014532,\n",
       "                         0.10475723109890978,\n",
       "                         0.10829113752172537],\n",
       "          train_eo_hist=[0.04674794516425873,\n",
       "                         0.0868002142160923,\n",
       "                         0.11924350164423458,\n",
       "                         0.1381134670272719,\n",
       "                         0.15445008720319098,\n",
       "                         0.16709221173235678,\n",
       "                         0.17291585593520498,\n",
       "                         0.17822368177605807,\n",
       "                         0.18177971770482992,\n",
       "                         0.18540162874171628,\n",
       "                         0.19201801074252228,\n",
       "                         0.195380894348119,\n",
       "                         0.20004786111250045,\n",
       "                         0.19827594269516213,\n",
       "                         0.19930534291856816,\n",
       "                         0.19935779902317785,\n",
       "                         0.19941472812895578,\n",
       "                         0.2005736420680067,\n",
       "                         0.19965362705498824,\n",
       "                         0.20172808300588935,\n",
       "                         0.1989881664720911,\n",
       "                         0.19846340210775948,\n",
       "                         0.1984589291065912,\n",
       "                         0.19825683078107953,\n",
       "                         0.19753444109240442,\n",
       "                         0.1983729254932196,\n",
       "                         0.19836397949088297,\n",
       "                         0.19712129843904463,\n",
       "                         0.19639890875036958,\n",
       "                         0.1960650602086289,\n",
       "                         0.19644912835439504,\n",
       "                         0.19468839243997743,\n",
       "                         0.19435901689940505,\n",
       "                         0.19584954287961243,\n",
       "                         0.19467497343647266,\n",
       "                         0.19362991770897764,\n",
       "                         0.19408209746344235,\n",
       "                         0.19427524978661753,\n",
       "                         0.19427301328603336,\n",
       "                         0.19342558288288175,\n",
       "                         0.1925781524797302,\n",
       "                         0.1914694581447049,\n",
       "                         0.19003586127027544,\n",
       "                         0.19133323492730764,\n",
       "                         0.19106973449484976,\n",
       "                         0.19022230409169816,\n",
       "                         0.19099938638556702,\n",
       "                         0.19086763616933805,\n",
       "                         0.19210137121883997,\n",
       "                         0.1922945235420151,\n",
       "                         0.19294209212023916,\n",
       "                         0.19365553580657763,\n",
       "                         0.19345791048223426,\n",
       "                         0.19358518769729488,\n",
       "                         0.1937124649123556,\n",
       "                         0.19305818683237913,\n",
       "                         0.19233803364428825,\n",
       "                         0.19317875454568745,\n",
       "                         0.1934355454763929,\n",
       "                         0.19174068467008976,\n",
       "                         0.19089101776635398,\n",
       "                         0.19114557219647538,\n",
       "                         0.1915318768428257,\n",
       "                         0.19022779368404102,\n",
       "                         0.18963715421159488,\n",
       "                         0.18898287613161846,\n",
       "                         0.18630883470593473,\n",
       "                         0.18422095975152886,\n",
       "                         0.18089711324703697,\n",
       "                         0.1795249184795537,\n",
       "                         0.17684864055328584,\n",
       "                         0.1731295434001071,\n",
       "                         0.16908778020810844,\n",
       "                         0.16445985054483184,\n",
       "                         0.15995919809661602,\n",
       "                         0.15780321153351154,\n",
       "                         0.15414998948844727,\n",
       "                         0.14938807310835767,\n",
       "                         0.14345382378571236,\n",
       "                         0.13797399071811595,\n",
       "                         0.13126042260101767,\n",
       "                         0.1262394787896384,\n",
       "                         0.12186610355648322,\n",
       "                         0.11683845024335163,\n",
       "                         0.1123355612945516,\n",
       "                         0.10386125726303566,\n",
       "                         0.0983791876948551,\n",
       "                         0.0924449383722098,\n",
       "                         0.08611991140204595,\n",
       "                         0.07758196876299964,\n",
       "                         0.07086616414531724,\n",
       "                         0.06193744385875238,\n",
       "                         0.05287697335595859,\n",
       "                         0.04537737694265491,\n",
       "                         0.03658040687231899,\n",
       "                         0.03155946306093976,\n",
       "                         0.02373943710940024,\n",
       "                         0.021058686181964117,\n",
       "                         0.01824842153888319,\n",
       "                         0.013877282806312147],\n",
       "          train_eodd_hist=[0.08350043830517129,\n",
       "                           0.0868002142160923,\n",
       "                           0.11924350164423458,\n",
       "                           0.1381134670272719,\n",
       "                           0.15445008720319098,\n",
       "                           0.16709221173235678,\n",
       "                           0.17291585593520498,\n",
       "                           0.17822368177605807,\n",
       "                           0.18177971770482992,\n",
       "                           0.18540162874171628,\n",
       "                           0.19201801074252228,\n",
       "                           0.195380894348119,\n",
       "                           0.20004786111250045,\n",
       "                           0.19827594269516213,\n",
       "                           0.19930534291856816,\n",
       "                           0.19935779902317785,\n",
       "                           0.19941472812895578,\n",
       "                           0.2005736420680067,\n",
       "                           0.19965362705498824,\n",
       "                           0.20172808300588935,\n",
       "                           0.1989881664720911,\n",
       "                           0.19846340210775948,\n",
       "                           0.1984589291065912,\n",
       "                           0.19825683078107953,\n",
       "                           0.19753444109240442,\n",
       "                           0.1983729254932196,\n",
       "                           0.19836397949088297,\n",
       "                           0.19712129843904463,\n",
       "                           0.19639890875036958,\n",
       "                           0.1960650602086289,\n",
       "                           0.19644912835439504,\n",
       "                           0.19468839243997743,\n",
       "                           0.19435901689940505,\n",
       "                           0.19584954287961243,\n",
       "                           0.19467497343647266,\n",
       "                           0.19362991770897764,\n",
       "                           0.19408209746344235,\n",
       "                           0.19427524978661753,\n",
       "                           0.19427301328603336,\n",
       "                           0.19342558288288175,\n",
       "                           0.1925781524797302,\n",
       "                           0.1914694581447049,\n",
       "                           0.19003586127027544,\n",
       "                           0.19133323492730764,\n",
       "                           0.19106973449484976,\n",
       "                           0.19022230409169816,\n",
       "                           0.19099938638556702,\n",
       "                           0.19086763616933805,\n",
       "                           0.19210137121883997,\n",
       "                           0.1922945235420151,\n",
       "                           0.19294209212023916,\n",
       "                           0.19365553580657763,\n",
       "                           0.19345791048223426,\n",
       "                           0.19358518769729488,\n",
       "                           0.1937124649123556,\n",
       "                           0.19305818683237913,\n",
       "                           0.19233803364428825,\n",
       "                           0.19317875454568745,\n",
       "                           0.1934355454763929,\n",
       "                           0.19174068467008976,\n",
       "                           0.19089101776635398,\n",
       "                           0.19114557219647538,\n",
       "                           0.1915318768428257,\n",
       "                           0.19022779368404102,\n",
       "                           0.18963715421159488,\n",
       "                           0.18898287613161846,\n",
       "                           0.18630883470593473,\n",
       "                           0.18422095975152886,\n",
       "                           0.18089711324703697,\n",
       "                           0.1795249184795537,\n",
       "                           0.17684864055328584,\n",
       "                           0.1731295434001071,\n",
       "                           0.16908778020810844,\n",
       "                           0.16445985054483184,\n",
       "                           0.15995919809661602,\n",
       "                           0.15780321153351154,\n",
       "                           0.15414998948844727,\n",
       "                           0.14938807310835767,\n",
       "                           0.14345382378571236,\n",
       "                           0.13797399071811595,\n",
       "                           0.13126042260101767,\n",
       "                           0.1262394787896384,\n",
       "                           0.12186610355648322,\n",
       "                           0.11683845024335163,\n",
       "                           0.1123355612945516,\n",
       "                           0.10386125726303566,\n",
       "                           0.0983791876948551,\n",
       "                           0.0924449383722098,\n",
       "                           0.08611991140204595,\n",
       "                           0.07758196876299964,\n",
       "                           0.07086616414531724,\n",
       "                           0.06193744385875238,\n",
       "                           0.05287697335595859,\n",
       "                           0.04537737694265491,\n",
       "                           0.03658040687231899,\n",
       "                           0.03192622512507015,\n",
       "                           0.03576522831050033,\n",
       "                           0.03928504860413362,\n",
       "                           0.042687200849137796,\n",
       "                           0.04558116174428928],\n",
       "          train_ei_hist=[0.008067539017242553,\n",
       "                         0.0006142790425661682,\n",
       "                         0.004383301707779896,\n",
       "                         0.002027324538406261,\n",
       "                         0.002595261358084966,\n",
       "                         0.0007752166190743148,\n",
       "                         0.00043996084929887047,\n",
       "                         0.0008831521739129933,\n",
       "                         0.0007581519384620705,\n",
       "                         0.0011682794430021293,\n",
       "                         0.00040418434973199524,\n",
       "                         0.00032742866019153105,\n",
       "                         0.0003100573088243852,\n",
       "                         0.00020100104180831302,\n",
       "                         0.00016778487047441892,\n",
       "                         0.000123019731227747,\n",
       "                         9.32043038340824e-05,\n",
       "                         6.0880903530069475e-05,\n",
       "                         0.000534161375881359,\n",
       "                         1.7629429394205864e-05,\n",
       "                         3.522863383353947e-05,\n",
       "                         5.2125413745440774e-05,\n",
       "                         6.356647495697754e-05,\n",
       "                         8.727190483837077e-05,\n",
       "                         0.0003853702905834977,\n",
       "                         0.00037002457462786964,\n",
       "                         0.0003403305290097247,\n",
       "                         0.00030503978779838903,\n",
       "                         0.00027802722798941915,\n",
       "                         0.0002509928160278063,\n",
       "                         0.00023782529271987318,\n",
       "                         0.00021357791362586198,\n",
       "                         0.00020060671946442454,\n",
       "                         0.0001995784069207529,\n",
       "                         0.00017941060721804458,\n",
       "                         0.00015953325128770235,\n",
       "                         0.00015620030524932105,\n",
       "                         0.00015340464778668128,\n",
       "                         0.00014860303856056856,\n",
       "                         0.00013827548348022134,\n",
       "                         0.00013008070500075686,\n",
       "                         0.00011788353711061905,\n",
       "                         0.00010049932611611023,\n",
       "                         0.00010320945825703642,\n",
       "                         9.506216596211203e-05,\n",
       "                         8.748222652754656e-05,\n",
       "                         8.668596103533766e-05,\n",
       "                         8.45978408559045e-05,\n",
       "                         8.940483886499262e-05,\n",
       "                         8.718942329599688e-05,\n",
       "                         8.653491301968685e-05,\n",
       "                         8.95918752003011e-05,\n",
       "                         8.385744234795656e-05,\n",
       "                         7.98445249665658e-05,\n",
       "                         7.589423203835377e-05,\n",
       "                         6.516961206159966e-05,\n",
       "                         5.296087395068039e-05,\n",
       "                         5.0704796673795016e-05,\n",
       "                         4.708960104637949e-05,\n",
       "                         3.0965520409154124e-05,\n",
       "                         1.8315064904017575e-05,\n",
       "                         1.202064545857695e-05,\n",
       "                         8.920527024836034e-06,\n",
       "                         4.42126826538658e-06,\n",
       "                         1.603109059999941e-05,\n",
       "                         2.602384555339121e-05,\n",
       "                         5.136939877348201e-05,\n",
       "                         7.05196735785174e-05,\n",
       "                         9.76600648462389e-05,\n",
       "                         0.00011167523188115869,\n",
       "                         0.000138150106863133,\n",
       "                         0.0005250410305748243,\n",
       "                         0.0002330742603754432,\n",
       "                         4.628239848247784e-05,\n",
       "                         4.616564906467779e-06,\n",
       "                         0.00037775291041908243,\n",
       "                         0.0004160280049979592,\n",
       "                         0.0005162967792575746,\n",
       "                         2.0474411428272887e-05,\n",
       "                         0.0001064238247969751,\n",
       "                         0.00010814901939970589,\n",
       "                         0.0008889784730665706,\n",
       "                         0.0010911977303087461,\n",
       "                         0.001460670055735025,\n",
       "                         0.0013741615666340579,\n",
       "                         0.0012595642070157842,\n",
       "                         0.0014823167606355936,\n",
       "                         0.0016705808855004278,\n",
       "                         0.0020421849980413187,\n",
       "                         0.0011840310326205472,\n",
       "                         0.0008156931487953711,\n",
       "                         0.0003596779304987363,\n",
       "                         0.0013732815522512487,\n",
       "                         0.0031552456788901395,\n",
       "                         0.004006221524294307,\n",
       "                         0.004246966347717485,\n",
       "                         0.005483745446779542,\n",
       "                         0.00681117420916022,\n",
       "                         0.009045086610261488,\n",
       "                         0.00999031191423938],\n",
       "          val_acc=0.0575,\n",
       "          val_dp=0.09434331977471833,\n",
       "          val_eo=0.01599263295219093,\n",
       "          val_eodd=0.03280761579994307,\n",
       "          val_ei=0.006747099054791317,\n",
       "          test_acc=0.069,\n",
       "          test_dp=0.0907521606507371,\n",
       "          test_eo=0.018154816211503763,\n",
       "          test_eodd=0.04068722455689244,\n",
       "          test_ei=0.0034028688658503947)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = logReg(num_features=dataset.XZ_train.shape[1])\n",
    "trainer_fb_fair(\n",
    "    model,\n",
    "    dataset,\n",
    "    optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4),\n",
    "    device,\n",
    "    100,\n",
    "    128,\n",
    "    False,\n",
    "    'SGD',\n",
    "    20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d747cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr__model_runner(dataset, hp, seeds):\n",
    "    test = {'accuracy':[],\n",
    "            'ei_disparity':[],\n",
    "            'dp_disparity':[],\n",
    "            'eo_disparity':[],\n",
    "            'eodd_disparity':[]}\n",
    "    \n",
    "    train = {'accuracy':[],\n",
    "            'ei_disparity':[],\n",
    "            'dp_disparity':[],\n",
    "            'eo_disparity':[],\n",
    "            'eodd_disparity':[]}\n",
    "    \n",
    "    val = {'accuracy':[],\n",
    "            'ei_disparity':[],\n",
    "            'dp_disparity':[],\n",
    "            'eo_disparity':[],\n",
    "            'eodd_disparity':[]}\n",
    "\n",
    "    def append_res(l,acc,ei,dp,eo,eodd):\n",
    "        l['accuracy'].append(acc)\n",
    "        l['ei_disparity'].append(ei)\n",
    "        l['dp_disparity'].append(dp)\n",
    "        l['eo_disparity'].append(eo)\n",
    "        l['eodd_disparity'].append(eodd)\n",
    "\n",
    "    for i in range(len(seeds)):\n",
    "        print('training seed', seeds[i] ,'started')\n",
    "        random.seed(seeds[i])\n",
    "        np.random.seed(seeds[i])\n",
    "        torch.manual_seed(seeds[i]) \n",
    "\n",
    "        model = logReg(num_features=dataset.XZ_train.shape[1])\n",
    "        model = model.to(device)\n",
    "        \n",
    "        lr = hp['learning_rate']\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        \n",
    "        results = trainer_kde_fair(\n",
    "            model,\n",
    "            dataset,\n",
    "            optimizer,\n",
    "            device,\n",
    "            n_epochs=hp['n_epochs'],\n",
    "            batch_size=hp['batch_size'], \n",
    "            z_blind=False,\n",
    "            fairness=hp['fairness'], \n",
    "            lambda_=hp['lambda_'], \n",
    "            h=hp['h'], \n",
    "            delta_huber=hp['delta_huber'], \n",
    "            optimal_effort=True, \n",
    "            delta_effort=hp['delta_effort']\n",
    "            )\n",
    "        \n",
    "        append_res(train,results.train_acc_hist[-1],results.train_ei_hist[-1],results.train_dp_hist[-1],results.train_eo_hist[-1],results.train_eodd_hist[-1])\n",
    "        append_res(val,results.val_acc,results.val_ei,results.val_dp,results.val_eo,results.val_eodd)\n",
    "        append_res(test,results.test_acc,results.test_ei,results.test_dp,results.test_eo,results.test_eodd)\n",
    "\n",
    "    def get_res(l):\n",
    "        res = {}\n",
    "        res['accuracy_mean'] = np.mean(l['accuracy'])\n",
    "        res['accuracy_var'] = np.std(l['accuracy'])\n",
    "        res['accuracy_list'] = l['accuracy']\n",
    "        res['ei_mean'] = np.mean(l['ei_disparity'])\n",
    "        res['ei_var'] = np.std(l['ei_disparity'])\n",
    "        res['ei_list'] = l['ei_disparity']\n",
    "        res['dp_mean'] = np.mean(l['dp_disparity'])\n",
    "        res['dp_var'] = np.std(l['dp_disparity'])\n",
    "        res['dp_list'] = l['dp_disparity']\n",
    "        res['eo_mean'] = np.mean(l['eo_disparity'])\n",
    "        res['eo_var'] = np.std(l['eo_disparity'])\n",
    "        res['eo_list'] = l['eo_disparity']\n",
    "        res['eodd_mean'] = np.mean(l['eodd_disparity'])\n",
    "        res['eodd_var'] = np.std(l['eodd_disparity'])\n",
    "        res['eodd_list'] = l['eodd_disparity']\n",
    "        return res\n",
    "\n",
    "    res_train = get_res(train)\n",
    "    res_val = get_res(val)\n",
    "    res_test = get_res(test)\n",
    "    print('Training finished for all seeds.')\n",
    "    \n",
    "    return res_train, res_val, res_test\n",
    "\n",
    "\n",
    "def experiment_runner(dataset, SGD_hp, EI_hp, DP_hp, EO_hp, EODD_hp, seeds):\n",
    "    \n",
    "    _, _, SGD = lr_kde_model_runner(dataset, SGD_hp, seeds)\n",
    "    _, _, EI = lr_kde_model_runner(dataset, EI_hp, seeds)\n",
    "    _, _, DP = lr_kde_model_runner(dataset, DP_hp, seeds)\n",
    "    _, _, EO = lr_kde_model_runner(dataset, EO_hp, seeds)\n",
    "    _, _, EODD = lr_kde_model_runner(dataset, EODD_hp, seeds)\n",
    "    \n",
    "    return SGD, EI, DP, EO, EODD\n",
    "\n",
    "def hyperparameter_test(dataset, hp_test, seed=0):\n",
    "    hp = hp_test.copy()\n",
    "    result = []\n",
    "    for i in hp_test['learning_rate']:\n",
    "        for k in hp_test['lambda_']:\n",
    "            c = []\n",
    "            hp['learning_rate'] = i\n",
    "            hp['lambda_'] = k\n",
    "            train, val, _ = lr_kde_model_runner(dataset, hp, seeds=[seed])\n",
    "            c.append(hp['learning_rate'])\n",
    "            c.append(hp['lambda_'])\n",
    "            c.append(train['accuracy_mean'])\n",
    "            c.append(val['accuracy_mean'])\n",
    "            c.append(val['ei_mean'])\n",
    "            c.append(val['dp_mean'])\n",
    "            c.append(val['eo_mean'])\n",
    "            c.append(val['eodd_mean'])\n",
    "            result.append(c)\n",
    "    print(tabulate(result, headers=['learning_rate', 'lambda_', 'accuracy_train', 'accuracy_val','ei', 'dp', 'eo', 'eodd']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
